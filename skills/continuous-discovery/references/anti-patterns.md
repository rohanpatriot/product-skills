# Anti-Patterns

Twenty-eight common mistakes in continuous discovery, organized by the phase where they occur. Each anti-pattern covers: what teams do wrong, why it fails, and how to fix it.

---

## Table of Contents

- [Outcome Anti-Patterns](#outcome-anti-patterns)
- [Interviewing Anti-Patterns](#interviewing-anti-patterns)
- [Opportunity Mapping Anti-Patterns](#opportunity-mapping-anti-patterns)
- [Prioritization Anti-Patterns](#prioritization-anti-patterns)
- [Ideation Anti-Patterns](#ideation-anti-patterns)
- [Assumption Anti-Patterns](#assumption-anti-patterns)
- [Testing Anti-Patterns](#testing-anti-patterns)

---

## Outcome Anti-Patterns

### 1. Accepting Output-Based Goals

**The mistake:** Accepting "ship feature X" or "launch by Q2" as the team's objective instead of a measurable outcome.

**Why it fails:** Output goals decouple the team from impact. You ship the feature, check the box, and never learn whether it mattered. The team has no reason to discover the best solution — any solution that ships satisfies the goal.

**The fix:** Negotiate with leadership to reframe output requests as outcome questions: "What business result do we expect this feature to drive?" Then own the outcome, not the output.

### 2. Choosing Outcomes Too Broad to Influence

**The mistake:** Accepting "increase revenue" or "grow users" as a team-level outcome.

**Why it fails:** A single product trio can't directly influence company-level metrics. Too many variables intervene. The team can't draw a clear line from their work to the result, so they can't learn or iterate effectively.

**The fix:** Decompose business outcomes into product outcomes the team can directly influence. Revenue depends on conversion, retention, expansion — pick the lever you can move.

### 3. Setting Only Performance Goals

**The mistake:** Setting only outcome targets (hit 15% activation rate) without learning goals (understand why users don't activate).

**Why it fails:** Performance goals without learning goals incentivize gaming the metric rather than understanding the problem. Teams that only chase numbers miss the underlying dynamics that drive sustainable improvement.

**The fix:** Pair every performance goal with a learning goal. "Hit 15% activation" AND "Understand the top three reasons users don't activate."

### 4. Changing Outcomes Too Frequently

**The mistake:** Switching the team's outcome every few weeks based on shifting leadership priorities.

**Why it fails:** Discovery takes time. Switching outcomes means abandoning partially-built understanding of the opportunity space. The team never builds deep enough knowledge to find non-obvious solutions.

**The fix:** Commit to an outcome for at least one quarter. If leadership shifts priorities, negotiate — explain what you've learned and the cost of switching.

---

## Interviewing Anti-Patterns

### 5. Leading the Witness

**The mistake:** Asking questions that suggest the answer: "Don't you think it would be easier if...?" or "Would you like a feature that does...?"

**Why it fails:** Customers are polite. They'll agree with leading questions to avoid conflict. You learn nothing about their actual experience — only that they're agreeable.

**The fix:** Ask about past behavior, not hypothetical preferences. "Tell me about the last time you..." not "Would you use...?"

### 6. Asking About Future Behavior

**The mistake:** Asking "Would you use X?" or "How much would you pay for Y?"

**Why it fails:** People are terrible at predicting their own future behavior. What they say they'll do and what they actually do are different things. Stated preference is not revealed preference.

**The fix:** Ask about current and past behavior. "How are you solving this today?" "What did you try last time?" Let real behavior inform your assumptions.

### 7. PM-Only Interviewing

**The mistake:** Only the product manager conducts interviews and "shares findings" with the rest of the trio.

**Why it fails:** Secondhand insights lose nuance. The PM filters through their own mental model and shares what they think matters. The designer misses body language cues about usability. The engineer misses clues about technical feasibility.

**The fix:** All three trio members attend interviews. If not every interview, rotate so each member attends at least twice per month.

### 8. Stopping Interviews When You "Have Enough"

**The mistake:** Doing a batch of interviews, deciding you understand the space, and stopping.

**Why it fails:** Understanding decays. Customer needs evolve. New segments emerge. Teams that stop interviewing slowly drift from customer reality and start making assumption-driven decisions again.

**The fix:** Keep interviewing every week, even when you feel you understand. Weekly interviews surface new opportunities, validate assumptions about known opportunities, and keep the team grounded in customer reality.

---

## Opportunity Mapping Anti-Patterns

### 9. Flat List of Opportunities

**The mistake:** Maintaining a flat list of opportunities instead of structuring them as a tree.

**Why it fails:** Flat lists become overwhelming as they grow. You can't prioritize effectively because you're comparing unrelated items. You miss the structure that reveals which areas are richest.

**The fix:** Structure opportunities hierarchically. Parent opportunities contain child opportunities. Prioritize at each level — compare siblings, not the entire list.

### 10. Mapping Solutions as Opportunities

**The mistake:** Putting solutions on the opportunity layer of the tree: "We need a better search function" instead of "Customers can't find what they're looking for."

**Why it fails:** Solutions-as-opportunities prematurely narrow the space. "Better search" forecloses alternatives like better navigation, better categorization, or better recommendations. The opportunity "can't find what they're looking for" opens up many possible solutions.

**The fix:** Frame opportunities as customer needs, pain points, or desires — not as solutions. Test: does this describe the customer's experience or a product feature?

### 11. Opportunity Space Too Thin

**The mistake:** Moving to solution generation with only 3-5 opportunities mapped.

**Why it fails:** A thin opportunity space means you're picking from a small menu. You may be missing the biggest opportunities entirely. Richer opportunity spaces lead to better target selection.

**The fix:** Continue interviewing until you have at least 10-15 distinct opportunities across multiple branches before prioritizing. The opportunity space should feel comprehensive, not rushed.

---

## Prioritization Anti-Patterns

### 12. Prioritizing by Loudest Voice

**The mistake:** Choosing the target opportunity based on who argues most passionately, not structured evaluation.

**Why it fails:** Passion doesn't correlate with customer evidence. The loudest voice may be anchored on an unrepresentative experience. Quiet team members with valid insights get drowned out.

**The fix:** Use structured criteria (frequency, intensity, connection to outcome, feasibility) and evaluate as a trio. Every voice counts equally when applying criteria.

### 13. Prioritizing by Ease Instead of Impact

**The mistake:** Choosing the opportunity that's easiest to address rather than the one most likely to move the outcome.

**Why it fails:** Easy opportunities are often easy because they're small. They move the needle slightly. Over time, always choosing easy leads to incremental improvements that never add up to meaningful progress.

**The fix:** Evaluate impact first, then feasibility. If the highest-impact opportunity is hard, that's worth more than an easy win on a low-impact opportunity.

### 14. Skipping Prioritization Entirely

**The mistake:** Trying to address multiple opportunities simultaneously instead of focusing on one.

**Why it fails:** Discovery requires focus. Addressing multiple opportunities dilutes attention, slows learning, and produces shallow solutions. One deeply explored opportunity yields better results than three superficially addressed ones.

**The fix:** Pick one target opportunity per discovery cycle. Commit to it for 2-4 weeks. If it doesn't pan out, switch — but switch deliberately, not reactively.

---

## Ideation Anti-Patterns

### 15. Group Brainstorming Without Individual Ideation

**The mistake:** Jumping straight into group discussion without individual thinking time first.

**Why it fails:** The first idea voiced anchors the group. Social dynamics suppress divergent thinking. Introverts don't contribute. You get variations on one theme instead of genuinely diverse ideas.

**The fix:** Always start with individual ideation (5-10 minutes, silent). Share round-robin after. Build on ideas after hearing all of them.

### 16. Evaluating Just One Solution

**The mistake:** Generating one solution and then evaluating whether it's "good enough."

**Why it fails:** Single-option evaluation lacks rigor. You can't assess trade-offs without alternatives. Cognitive biases (endowment effect, confirmation bias) make you overvalue the one idea you have.

**The fix:** Generate at least three solutions. Compare them side by side. The comparison itself improves thinking, even if you end up choosing the first idea.

### 17. Falling in Love With Solutions

**The mistake:** Becoming emotionally attached to a solution and defending it against evidence.

**Why it fails:** Attachment prevents learning. When evidence shows a solution is flawed, attached teams rationalize instead of pivoting. They spend more time defending than discovering.

**The fix:** Hold solutions loosely. They're hypotheses, not children. Set clear criteria for success and failure before testing. If a solution fails its tests, celebrate the learning and move on.

---

## Assumption Anti-Patterns

### 18. Skipping Assumption Identification

**The mistake:** Moving from solution selection directly to building without identifying and testing assumptions.

**Why it fails:** Every solution contains hidden assumptions — about desirability, feasibility, viability, usability, and ethics. Skipping assumption identification means you discover these assumptions after building, when the cost of being wrong is highest.

**The fix:** After selecting a solution, systematically walk through it and ask "what has to be true for this to work?" at each step. Use the five assumption categories as prompts.

### 19. Testing Easy Assumptions First

**The mistake:** Testing the assumptions you're most confident about instead of the ones you're least confident about.

**Why it fails:** You're generating evidence for things you already believe. The leap-of-faith assumptions — the ones that would kill the solution if false — remain untested until you've already invested heavily.

**The fix:** Use assumption mapping to identify leap-of-faith assumptions (high importance, low evidence). Test those first. If they fail, you've saved the effort of testing everything else.

### 20. Conflating Opinions With Assumptions

**The mistake:** Treating team preferences as assumptions to test. "We assume users will prefer blue over green."

**Why it fails:** Preferences are rarely make-or-break assumptions. Testing them consumes time that should be spent on structural assumptions about desirability, viability, or feasibility.

**The fix:** Focus on assumptions that would kill the solution if false. "Users will switch from their current tool" is a critical assumption. "Users prefer blue buttons" is not.

---

## Testing Anti-Patterns

### 21. Building to Test

**The mistake:** Building the full solution (or a significant portion) to test whether it works.

**Why it fails:** Building is the most expensive way to learn. If assumptions are wrong, you've wasted engineering time. Simulated tests (prototypes, fake doors, Wizard of Oz) can test the same assumptions in a fraction of the time.

**The fix:** Default to simulated environments. Ask "Can I test this without code?" first. Only build when the assumption specifically requires real functionality to test.

### 22. Testing the Whole Idea

**The mistake:** Running a test that evaluates the entire solution rather than a specific assumption.

**Why it fails:** When a holistic test fails, you don't know which assumption was wrong. When it succeeds, you don't know which assumptions were validated. You can't learn or iterate effectively from ambiguous results.

**The fix:** Test one assumption at a time. Design each test to produce clear evidence for or against a specific claim. Use the result to update your assumption map.

### 23. No Success Criteria Before Testing

**The mistake:** Running a test without defining what success and failure look like in advance.

**Why it fails:** Without pre-set criteria, confirmation bias takes over. Ambiguous results get interpreted as positive. The team moves forward on weak evidence because they wanted the solution to work.

**The fix:** Before running any test, define: "We will consider this assumption validated if [specific measurable result]. We will consider it invalidated if [specific measurable result]."

### 24. Ignoring Negative Results

**The mistake:** Rationalizing away results that disconfirm your assumptions. "The sample was too small." "Users didn't understand the test." "We'll fix it in the real version."

**Why it fails:** The whole point of testing is to find out when you're wrong. Ignoring negative results turns testing into a rubber stamp and restores opinion-driven decision making.

**The fix:** Set criteria before testing. If results meet failure criteria, accept the result. Celebrate the learning. Pivot to another solution or revisit the opportunity.

### 25. A/B Testing as Discovery

**The mistake:** Using A/B tests as the primary assumption testing method.

**Why it fails:** A/B tests require building the thing you're testing. They're the most expensive assumption tests. They work for optimization but are too slow and costly for discovery, where you need to test many assumptions rapidly across multiple solutions.

**The fix:** Use A/B tests for optimization of validated solutions. For discovery, use faster methods: prototypes, fake doors, concierge tests, one-question surveys, Wizard of Oz.

### 26. Testing Without Customer Contact

**The mistake:** Running tests that don't involve real customers — internal reviews, stakeholder feedback, team opinions.

**Why it fails:** Internal users are not customers. They have different knowledge, motivations, and context. Stakeholder approval does not predict customer behavior. Only customer behavior predicts customer behavior.

**The fix:** Every assumption test must involve real customers or realistic proxies. Internal feedback is useful for feasibility assumptions but not desirability or usability assumptions.

### 27. One-and-Done Testing

**The mistake:** Running a single test and considering the assumption fully validated or invalidated.

**Why it fails:** Single tests can be misleading. Sample bias, question design, and test execution can all skew results. Important assumptions deserve converging evidence from multiple test types.

**The fix:** For critical assumptions, triangulate — test with different methods and see if results converge. A prototype test and a survey pointing the same direction is stronger than either alone.

### 28. Separating Discovery From Delivery

**The mistake:** Running discovery as a separate phase or track, disconnected from the delivery cadence.

**Why it fails:** Separate discovery produces handoff documents instead of shared understanding. The delivery team builds without the context that discovery generated. The discovery team moves on without seeing their insights realized. Learning is lost at the seam.

**The fix:** The same trio does both discovery and delivery, in parallel, every week. Discovery insights flow directly into the delivery backlog. Delivery feedback flows back into the Opportunity Solution Tree.
